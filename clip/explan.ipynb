{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.clip的训练方式为对比学习，论文中图2的bag of words prediction和tansformer language model代表什么？\n",
    "\n",
    "2.图像编码器和文本编码器的forward过程。\n",
    "\n",
    "3.如何对CLIP进行finetune。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLIP的输入包括两部分，image和text，分别对其进行预处理后得到送入到网络结构中的embedding。\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 图像预处理 \n",
    "'''\n",
    "图像变换函数，对缩放后的图像进行随机正方形裁剪是训练过程中唯一的数据增强方法。\n",
    "\n",
    "Compose(\n",
    "step 1: Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
    "step 2: CenterCrop(size=(224, 224))\n",
    "step 3: <function _convert_image_to_rgb at 0x7f4e27b1f0d0>\n",
    "step 4: ToTensor()\n",
    "        Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))   \n",
    ")\n",
    "\n",
    "step 5: Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
    "<shape: torch.Size([3, 224, 224])>\n",
    "正则化：(pixel_value - mean)/std\n",
    "\n",
    "step 6:unsqueeze(0)\n",
    "<shape: torch.Size([1, 3, 224, 224])>\n",
    "维度扩充：CHW 扩充一维变为NCHW。\n",
    "''' \n",
    "image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本预处理\n",
    "'''\n",
    "1.使用tokenizer将text进行分词处理，根据vocabulary获得对应id；\n",
    "可以通过clip.tokenize()调用不区分大小写的分词器，\n",
    "使用到了BPE(Byte-Pair Encoding)。\n",
    "默认情况下，输出被填充为77个tokens(用0进行填充，表示的字符为\"!\")。\n",
    "\n",
    "2. 根据词表id索引到对应的token_embedding。\n",
    "'''\n",
    "\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图像编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \"\"\"Subclass torch's LayerNorm to handle fp16.\"\"\"\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)\n",
    "\n",
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x * torch.sigmoid(1.702 * x)\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "\n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ViT   model.encode_image()调用模型 \n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "        # width相当于transform中的d_model\n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    " \n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    " \n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    " \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x:[1,3,224,224]\n",
    "        x = self.conv1(x)  # shape = [*, width, grid, grid] # 将图片分成[32,32]个patch [1,768,7,7]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2],合并高宽 [1,768,49]\n",
    "        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width] ，更换位置 [1,49,768]\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width],添加cls token[1,50,768]\n",
    "        x = x + self.positional_embedding.to(x.dtype)  # 这里位置编码是可学习的参数，可能是切了path顺序让模型自己学习吧  [1,50,768]\n",
    "        x = self.ln_pre(x)  # [1,50,768]\n",
    " \n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND  # [pixel,b,d_model]=[50,1,768]\n",
    "        x = self.transformer(x)  # 多头transformer [50,1,768]\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD  # [1,50,768]\n",
    " \n",
    "        x = self.ln_post(x[:, 0, :])  # x[:, 0, :] 将所有信息汇聚到cls token中，只需前面来做下游任务 [1,768]\n",
    " \n",
    "        if self.proj is not None:  # self.proj是可学习参数，维度为[768,512]\n",
    "            x = x @ self.proj  # 通过学习参数将维度再次融合变成512特征，最终为[1,512]\n",
    " \n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP():\n",
    "\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                  # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "        vision_heads = vision_width // 64\n",
    "\n",
    "        '''\n",
    "        图像token序列加入一个extra learnable [class] embedding, \n",
    "        [class] token学习到的embedding作为图像的输出embedding,与原始vit论文类似。\n",
    "        另外，图像编码器也可以使用ResNet，但是clip中对原始的ResNet有较大幅度的修改，详情看代码。\n",
    "        '''\n",
    "\n",
    "        self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "\n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask=self.build_attention_mask()\n",
    "        )\n",
    "        # 将单词（token）转换为密集的词嵌入向量\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)   \n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "        # 文本特征转换，可学习的参数\n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))   \n",
    "    \n",
    "    # 图片编码特征\n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))\n",
    "\n",
    "\n",
    "# BERT\n",
    "    # 文本编码 [batch_size(句子数), n_ctx(句子中的单词数，不够补0)] [3, 77]\n",
    "    def encode_text(self, text):   \n",
    "        # x 每个句子前面有[CLS]，最后有[Seq]\n",
    "        x = self.token_embedding(text).type(self.dtype)  # 维度表示 [3，77，512]表示： [batch_siz(句子数), n_ctx(句子中的单词数，不够补0), d_model(嵌入层维度)] \n",
    "\n",
    "        # 可学习的位置编码，[3, 77, 512] \n",
    "        x = x + self.positional_embedding.type(self.dtype) \n",
    "\n",
    "        x = x.permute(1, 0, 2)  # NLD -> LND [77, 3, 512]\n",
    "        x = self.transformer(x)   # Transformer encoder [77, 3, 512]\n",
    "        x = x.permute(1, 0, 2)  # LND -> NLD  [3, 77, 512]\n",
    "        x = self.ln_final(x).type(self.dtype)  # LN层\n",
    "    \n",
    "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
    "        # 结束符\"<|endoftext|>\"的编号最大，得到该位置的embedding\n",
    "        # 获取每个句子最后一个seq字段，seq是最大的，因此能获得句子中的单词数    【【take features from the eot embedding (eot_token is the highest number in each sequence)\n",
    "        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection   # 矩阵乘法\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention_mask\n",
    "def build_attention_mask(self):\n",
    "    # lazily create causal attention mask, with full attention between the vision tokens\n",
    "    # pytorch uses additive attention mask; fill with -inf\n",
    "    mask = torch.empty(self.context_length, self.context_length)\n",
    "    mask.fill_(float(\"-inf\"))\n",
    "    mask.triu_(1)  # zero out the lower diagonal\n",
    "    return mask\n",
    "\n",
    "'''\n",
    "当self.context_length = 5时，返回得到的mask如下，\n",
    "tensor([[0., -inf, -inf, -inf, -inf],\n",
    "        [0.,   0., -inf, -inf, -inf],\n",
    "        [0.,   0.,   0., -inf, -inf],\n",
    "        [0.,   0.,   0.,   0., -inf],\n",
    "        [0.,   0.,   0.,   0.,   0.]])\n",
    "这是一个下三角矩阵，阻止对某些位置的attention，使得每个token只关注Transformer的自注意力层中的左侧标记。\n",
    "\n",
    "以text = \"Hello World!\"为例，下方矩阵代表attention map，'v'代表非0值。\n",
    "                  开始    hello  world    !     结束\n",
    "         tensor([[49406,  3306,  1002,   256, 49407]]) \n",
    "tensor([[49406,     v       0      0      0      0\n",
    "          3306,     v       v      0      0      0\n",
    "          1002,     v       v      v      0      0\n",
    "           256,     v       v      v      v      0\n",
    "         49407]])   v       v      v      v      v\n",
    "\n",
    "attention的mask机制屏蔽了token对其右侧邻居token的感受野，\n",
    "\"!\"可以学习到\"Hello\"和\"World\"的特征，\n",
    "而\"Hello\"只能学习到开始符和自身的特征。\n",
    "这是从左到右的单向embedding学习，并且以结束符的embedding作为整个text的输出embedding。\n",
    "\n",
    "# text_projection\n",
    "self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
