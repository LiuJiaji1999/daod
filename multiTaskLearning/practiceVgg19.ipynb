{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多任务学习 multi-task learning\n",
    "1. 多任务学习能够同时处理和学习多个相关任务，将多个互相关联的任务联合训练，从而提高模型的泛化能力和效果，使用一个或多个输入用于预测几个不同输出。\n",
    "2. 在多任务学习中，多个不同的任务使用同一网络模型，模型同时学习多个任务的知识，从而更好地利用训练数据进行训练。例如，在自动驾驶中，模型需要识别障碍物、规划路线、提供适量的油门制动和转向等，通过考虑相同的输入集(来自多个传感器)实时完成这些任务。\n",
    "3. 模型分析，我们将学习如何在单个前向传递中同时进行连续值预测和离散值(类别)预测。构建模型策略如下:\n",
    "    - 导入相关库\n",
    "    - 获取包含人物图像、性别和年龄信息的数据集\n",
    "    - 预处理数据并创建训练和测试数据集\n",
    "    - 构建模型:\n",
    "        1. 特征提取层使用预训练VGG19模型\n",
    "        2. 创建两个分支独立层，其中一层对应于年龄估计，另一层对应于性别分类\n",
    "        3. 每个输出分支都有不同的损失函数，年龄是一个连续值(计算 mse 或 mae 损失)，而性别D是一个分类值(计算交叉熵损失)\n",
    "        4. 对年龄估计损失和性别分类损失加权求\n",
    "        5. 通过反向传播优化权重值来最小化整体损失\n",
    "    - 训练模型并预测新图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np, cv2, pandas as pd, time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了构建多分类模型，我们将使用 FairFace 数据集， FairFace 数据集是一个用于面部分析和人脸识别的多样性人脸数据集，由Fairface 团队于 2019 年发布。 \n",
    "\n",
    "FairFace 数据集的目标是提供一个包含多种族、性别和年龄的真实世界人脸图像集合，以便研究人员可以训练和评估面部识别算法在多样性群体中的表现。该数据集包含了 87,022 个标记的人脸图像，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          file  age  gender        race  service_test\n",
      "0  train/1.jpg   59    Male  East Asian          True\n",
      "1  train/2.jpg   39  Female      Indian         False\n",
      "2  train/3.jpg   11  Female       Black         False\n",
      "3  train/4.jpg   26  Female      Indian          True\n",
      "4  train/5.jpg   26  Female      Indian          True\n"
     ]
    }
   ],
   "source": [
    "trn_df = pd.read_csv('fairface-labels-train.csv')\n",
    "val_df = pd.read_csv('fairface-labels-val.csv')\n",
    "print(trn_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 224\n",
    "class GenderAgeClass(Dataset):\n",
    "    def __init__(self, df, tfms=None):\n",
    "        self.df = df\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "    # 返回输入图像数量\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    # 获取给定位置 ix 的图像信息\n",
    "    def __getitem__(self, ix):\n",
    "        f = self.df.iloc[ix].squeeze()\n",
    "        file = f.file\n",
    "        gen = f.gender == 'Female'\n",
    "        age = f.age\n",
    "        im = cv2.imread(file)\n",
    "        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        return im, age, gen\n",
    "    \n",
    "    # 图像预处理，大小、通道、归一化\n",
    "    def preprocess_image(self, im):\n",
    "        im = cv2.resize(im, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "        im = torch.tensor(im).permute(2,0,1)  # 本身维度 是 （0 1 2）\n",
    "        im = self.normalize(im/255.)\n",
    "        return im[None]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        'preprocess images, ages and genders'\n",
    "        ims, ages, genders = [], [], []\n",
    "        for im, age, gender in batch:\n",
    "            im = self.preprocess_image(im) # 处理图像\n",
    "            ims.append(im)\n",
    "\n",
    "            ages.append(float(int(age)/80)) # 缩放年龄 /最大年龄80,值变为0-1（目的：避免梯度消失，在后处理期间还原）\n",
    "            genders.append(float(gender)) # 性别转换为 浮点值\n",
    "\n",
    "        # 都转换为 张量并返回\n",
    "        ages, genders = [torch.tensor(x).to(device).float() for x in [ages, genders]]\n",
    "        ims = torch.cat(ims).to(device)\n",
    "\n",
    "        return ims, ages, genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread('train/3204.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b6469806141e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ObjectDetection/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    650\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    653\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ObjectDetection/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ObjectDetection/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ObjectDetection/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-72172bf7a88c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.9.0) /io/opencv/modules/imgproc/src/color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "trn = GenderAgeClass(trn_df) # 训练集\n",
    "val = GenderAgeClass(val_df) # 验证集\n",
    "\n",
    "train_loader = DataLoader(trn, batch_size=32, shuffle=True, drop_last=True, collate_fn=trn.collate_fn)\n",
    "test_loader = DataLoader(val, batch_size=32, collate_fn=val.collate_fn)\n",
    "a,b,c, = next(iter(train_loader))\n",
    "print(b)\n",
    "print(a.shape, b.shape, c.shape)\n",
    "# torch.Size([32, 3, 224, 224]) torch.Size([32]) torch.Size([32])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = models.vgg16(pretrained = True)\n",
    "\n",
    "    # 冻结加载模型的参数\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  \n",
    "    \n",
    "    # 使用自定义网络 替换avgpool层\n",
    "    model.avgpool = nn.Sequential(\n",
    "        nn.Conv2d(512,512, kernel_size=3),\n",
    "        nn.MaxPool2d(2),\n",
    "        nn.ReLU(),\n",
    "        nn.Flatten()\n",
    "    )\n",
    "\n",
    "    # 创建包含两个输出分支的分类器 \n",
    "    class ageGenderClassifier(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(ageGenderClassifier, self).__init__()\n",
    "\n",
    "            # 定义中间层\n",
    "            self.intermediate = nn.Sequential(\n",
    "                nn.Linear(2048,512),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(512,128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.4),\n",
    "                nn.Linear(128,64),\n",
    "                nn.ReLU(),\n",
    "            )\n",
    "\n",
    "            self.age_classifier = nn.Sequential( \n",
    "                nn.Linear(64, 1),\n",
    "                nn.Sigmoid() # 年龄输出介于0-1\n",
    "            )\n",
    "            self.gender_classifier = nn.Sequential(\n",
    "                nn.Linear(64, 1),\n",
    "                nn.Sigmoid() # 性别输出是0/1\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.intermediate(x)\n",
    "            age = self.age_classifier(x)\n",
    "            gender = self.gender_classifier(x)\n",
    "            return gender, age\n",
    "\n",
    "    model.classifier = ageGenderClassifier()\n",
    "    \n",
    "    gender_criterion = nn.BCELoss() # 性别分类的二元交叉熵损失\n",
    "    age_criterion = nn.L1Loss() #年龄分类的L1损失\n",
    "    loss_functions = gender_criterion, age_criterion\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr= 1e-4)\n",
    "    \n",
    "    return model.to(device), loss_functions, optimizer\n",
    "\n",
    "# 初始化变量中的值\n",
    "model, loss_functions, optimizer = get_model()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像、性别、年龄、模型、优化器、loss 作为输入计算总损失\n",
    "def train_batch(data, model, optimizer, criteria):\n",
    "    model.train()\n",
    "    ims, age, gender = data\n",
    "    optimizer.zero_grad() # 优化器重置\n",
    "    pred_gender, pred_age = model(ims)  # 计算性别、年龄的预测值\n",
    "    \n",
    "    gender_criterion, age_criterion = criteria\n",
    "    # 压缩预测形状为（batch_size,1），整形为和目标值相同形状 batch_size\n",
    "    gender_loss = gender_criterion(pred_gender.squeeze(), gender) \n",
    "    age_loss = age_criterion(pred_age.squeeze(), age)\n",
    "    \n",
    "    total_loss = gender_loss + age_loss\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    return total_loss\n",
    "\n",
    "def validate_batch(data, model, criteria):\n",
    "    # 模型评估阶段，不需要 梯度计算\n",
    "    model.eval()\n",
    "    ims, age, gender = data\n",
    "    with torch.no_grad():\n",
    "        pred_gender, pred_age = model(ims)\n",
    "    \n",
    "    gender_criterion, age_criterion = criteria\n",
    "    gender_loss = gender_criterion(pred_gender.squeeze(), gender)\n",
    "    age_loss = age_criterion(pred_age.squeeze(), age)\n",
    "\n",
    "    total_loss = gender_loss + age_loss\n",
    "    pred_gender = (pred_gender > 0.5).squeeze()\n",
    "    gender_acc = (pred_gender == gender).float().sum()\n",
    "    age_mae = torch.abs(age - pred_age).float().sum()\n",
    "    return total_loss, gender_acc, age_mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练模型，\n",
    "model, criteria, optimizer = get_model()\n",
    "# 定义存储 训练和测试 损失值的列表，并制定 epoch 数\n",
    "val_gender_accuracies = []\n",
    "val_age_maes = []\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "n_epochs = 10\n",
    "best_test_loss = 1000\n",
    "start = time.time()\n",
    "\n",
    "# 每个epoch 开始前 重新初始化训练和测试的损失值\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_train_loss, epoch_test_loss = 0, 0\n",
    "    val_age_mae, val_gender_acc, ctr = 0, 0, 0\n",
    "    _n = len(train_loader)\n",
    "    \n",
    "    for ix, data in enumerate(train_loader):\n",
    "        # if ix == 100: break\n",
    "        loss = train_batch(data, model, optimizer, criteria)\n",
    "        epoch_train_loss += loss.item()\n",
    "\n",
    "    for ix, data in enumerate(test_loader):\n",
    "        # if ix == 10: break\n",
    "        loss, gender_acc, age_mae = validate_batch(data, model, criteria)\n",
    "        epoch_test_loss += loss.item()\n",
    "        val_age_mae += age_mae\n",
    "        val_gender_acc += gender_acc\n",
    "        ctr += len(data[0]) # 测试数据总量\n",
    "\n",
    "    val_age_mae /= ctr\n",
    "    val_gender_acc /= ctr\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    epoch_test_loss /= len(test_loader)\n",
    "\n",
    "    # 输出每个epoch结束时的性能指标\n",
    "    elapsed = time.time()-start\n",
    "    best_test_loss = min(best_test_loss, epoch_test_loss)\n",
    "    print('{}/{} ({:.2f}s - {:.2f}s remaining)'.format(epoch+1, n_epochs, time.time()-start, (n_epochs-epoch)*(elapsed/(epoch+1))))\n",
    "    info = f'''Epoch: {epoch+1:03d}\\tTrain Loss: {epoch_train_loss:.3f}\\tTest: {epoch_test_loss:.3f}\\tBest Test Loss: {best_test_loss:.4f}'''\n",
    "    info += f'\\nGender Accuracy: {val_gender_acc*100:.2f}%\\tAge MAE: {val_age_mae:.2f}\\n'\n",
    "    print(info)\n",
    "\n",
    "    # 存储 每个epoch中 测试数据集的年龄和性别预测的准确率\n",
    "    val_gender_accuracies.append(val_gender_acc)\n",
    "    val_age_maes.append(val_age_mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.arange(1,len(val_gender_accuracies)+1)\n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax = ax.flat\n",
    "ax[0].plot(epochs, val_gender_accuracies, 'bo')\n",
    "ax[1].plot(epochs, val_age_maes, 'r')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('Accuracy')\n",
    "ax[1].set_ylabel('MAE')\n",
    "ax[0].set_title('Validation Gender Accuracy')\n",
    "ax[0].set_title('Validation Age Mean-Absolute-Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机选择测试图像，预测图中人物年龄和性别\n",
    "im = cv2.imread('val/4.jpg')\n",
    "im = trn.preprocess_image(im).to(device)\n",
    "\n",
    "gender, age = model(im)\n",
    "pred_gender = gender.to('cpu').detach().numpy()\n",
    "pred_age = age.to('cpu').detach().numpy()\n",
    "\n",
    "im = cv2.imread('val/4.jpg')\n",
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(im)\n",
    "plt.show()\n",
    "print('predicted gender:',np.where(pred_gender[0][0]<0.5,'Male','Female'), '; Predicted age', int(pred_age[0][0]*80))\n",
    "# predicted gender: Female ; Predicted age 26\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Explan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
